{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Vary batch size.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyOtCHs0ZeDC05bTv2m3GduM"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"id":"qCYSVjCgxP9G"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"XVf2IpNhxokr"},"source":["# Imports / paths"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"o6G4MUI7xrxy","executionInfo":{"status":"ok","timestamp":1619530915073,"user_tz":240,"elapsed":4727,"user":{"displayName":"Alex Palermo","photoUrl":"","userId":"06488501085160491296"}},"outputId":"f346c215-3937-46b2-a6ba-b87ce2a783e2"},"source":["! pip install torch-summary"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Collecting torch-summary\n","  Downloading https://files.pythonhosted.org/packages/ca/db/93d18c84f73b214acfa4d18051d6f4263eee3e044c408928e8abe941a22c/torch_summary-1.4.5-py3-none-any.whl\n","Installing collected packages: torch-summary\n","Successfully installed torch-summary-1.4.5\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-ln3x-Zdx2Sc","executionInfo":{"status":"ok","timestamp":1619530950677,"user_tz":240,"elapsed":40323,"user":{"displayName":"Alex Palermo","photoUrl":"","userId":"06488501085160491296"}},"outputId":"80b35901-508b-4c43-fa06-3bcc51e5a7b7"},"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from torch.utils.data import Dataset, DataLoader, random_split\n","import pandas as pd\n","import json\n","from torchsummary import summary\n","import time\n","from sklearn.preprocessing import LabelEncoder\n","from sklearn.metrics import precision_recall_fscore_support\n","import librosa.display\n","import math\n","import os\n","from textwrap import dedent\n","\n","IRMAS_PATH = \"/content/drive/MyDrive/ITCS 5156 project/IRMAS dataset/IRMAS-TrainingData/\"\n","JSON_PATH = \"/content/drive/MyDrive/ITCS 5156 project/IRMAS dataset/json_files/\"\n","filename = \"irmas_data_mfcc13_hop_length256_n_fft2048.json\"\n","\n","from google.colab import drive\n","drive.mount(\"/content/drive\", force_remount=True)"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"aSwNPE4kyA9t"},"source":["# Dataset / Preprocessing"]},{"cell_type":"code","metadata":{"id":"AaLYxQywx2-p","executionInfo":{"status":"ok","timestamp":1619530952462,"user_tz":240,"elapsed":234,"user":{"displayName":"Alex Palermo","photoUrl":"","userId":"06488501085160491296"}}},"source":["class IRMASDataset(Dataset):\n","  def __init__(self, JSON_PATH=JSON_PATH, filename=filename, transform=None):\n","    with open(JSON_PATH + filename, \"r\") as f:\n","      self.irmas_data = json.load(f)\n","    self.metadata = self.irmas_data.pop('metadata') \n","    self.instruments = [\"cel\", \"cla\", \"flu\", \"gac\", \"gel\", \"org\", \"pia\", \"sax\",\n","    \"tru\", \"vio\", \"voi\"]\n","    self.encoder = LabelEncoder()\n","    self.encoder.fit(self.instruments)\n","\n","  def __len__(self):\n","    return len(self.irmas_data)\n","\n","  def __getitem__(self, idx):\n","    if torch.is_tensor(idx):\n","      idx = np.array(idx.tolist())\n","\n","    mfccs = np.array(self.irmas_data[str(idx)]['mfccs'])[np.newaxis,...]\n","\n","    primary_instrument = self.encoder.transform(\n","      [np.array(self.irmas_data[str(idx)]['primary_instrument'])]\n","    )\n","    sample = {'mfccs': mfccs, 'instrument': primary_instrument, 'metadata': self.metadata}\n","    return sample\n","\n","def prep_dataset(filename=filename, val_split=0.2, batch_size=1):\n","\n","  dataset = IRMASDataset(JSON_PATH=JSON_PATH, filename=filename)\n","  train_set, val_set = random_split(dataset, [round(len(dataset) * (1-val_split)), round(len(dataset)*val_split)])\n","\n","  train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True, num_workers=1)\n","  val_loader = DataLoader(val_set, batch_size=batch_size, shuffle=True, num_workers=1)\n","\n","  return train_loader, val_loader, dataset"],"execution_count":3,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"E8kVKwZ9yJIe"},"source":["# Architectures"]},{"cell_type":"code","metadata":{"id":"zSK4dT7EyGzi","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1619530954418,"user_tz":240,"elapsed":1127,"user":{"displayName":"Alex Palermo","photoUrl":"","userId":"06488501085160491296"}},"outputId":"e2eae782-6b78-4d14-f4ec-10b68de30b41"},"source":["################################################################################\n","#################################### BLOCKS ####################################\n","################################################################################\n","\n","class ConvBlock(nn.Module):\n","\n","\tdef __init__(self, in_channels, out_channels, conv_kernel_size=3,\n","\t\t\t\t\t\t\t conv_stride=1, conv_padding=0,\n","\t\t\t\t\t\t\t inc_pool=True, pool_kernel_size=2, pool_stride=2):\n","\t\t\"\"\"Convolutional block with conv2d, linear activation, max pooling, \n","\t\t\tand batch norm\n","\t\t:param in_channels:\n","\t\t:param out_channels:\n","\t\t:param conv_kernel_size:\n","\t\t:param conv_stride:\n","\t\t:param conv_padding:\n","\t\t:param inc_pool: If true, includes a max pooling layer\n","\n","\t\tThe following params only matter if inc_pool is True\n","\t\t:param pool_kernel_size:\n","\t\t:param pool_stride:\n","\t\t\"\"\"\n","\t\tsuper(ConvBlock, self).__init__()\n","\n","\t\t# construct sequential blocks\n","\t\tif inc_pool:\n","\t\t\tself.conv_block = nn.Sequential(\n","\t\t\t\t\t\tnn.Conv2d(in_channels=in_channels, out_channels=out_channels,\n","\t\t\t\t\t\t\t\t\t\tkernel_size=conv_kernel_size, stride=conv_stride,\n","\t\t\t\t\t\t\t\t\t\t\tpadding=conv_padding),\n","\t\t\t\t\t\tnn.ReLU(),\n","\t\t\t\t\t\tnn.MaxPool2d(kernel_size=pool_kernel_size, stride=pool_stride),\n","\t\t\t\t\t\tnn.BatchNorm2d(num_features=out_channels)\n","\t\t\t\t)\n","\t\telse:\n","\t\t\tself.conv_block = nn.Sequential(\n","\t\t\t\t\t\tnn.Conv2d(in_channels=in_channels, out_channels=out_channels,\n","\t\t\t\t\t\t\t\t\t\t\tkernel_size=conv_kernel_size, stride=conv_stride,\n","\t\t\t\t\t\t\t\t\t\t\tpadding=conv_padding),\n","\t\t\t\t\t\tnn.ReLU(),\n","\t\t\t\t\t\tnn.BatchNorm2d(num_features=out_channels)\n","\t\t\t\t) \n","\n","\t# run forward\n","\tdef forward(self, x):\n","\t\tx = self.conv_block(x)\n","\t\treturn x\n","\n","class LinearBlock(nn.Module):\n","\n","\tdef __init__(self, in_features, out_features, dropout_prob=0):\n","\t\t\"\"\"Linear block with dense layer, relu, batch norm, then dropout\n","\t\t:param in_features:\n","\t\t:param out_features:\n","\t\t:param dropout_prob: Set to 0 for no dropout layer\n","\t\t\"\"\"\n","\n","\t\tsuper(LinearBlock, self).__init__()\n","\t\tself.linear_block = nn.Sequential(\n","\t\t\t\tnn.Linear(in_features=in_features, out_features=out_features),\n","\t\t\t\tnn.ReLU(),\n","\t\t\t\tnn.BatchNorm1d(num_features=out_features),\n","\t\t\t\tnn.Dropout(p=dropout_prob)\n","\t\t)\n","\n","\tdef forward(self, x):\n","\t\tx = self.linear_block(x)\n","\t\treturn x\n","\n","class HeadBlock(nn.Module):\n","\n","\tdef __init__(self, in_features):\n","\t\t\"\"\"Linear block with softmax output.\n","\t\tNOTE: no longer using softmax output since the CrossEntropyLoss handles that\n","\t\t:param in_features:\n","\t\tout_features is fixed to 11 to corrospond to the number of classes\n","\t\t\"\"\"\n","\t\tsuper(HeadBlock, self).__init__()\n","\t\tself.head_block = nn.Sequential(\n","\t\t\t\tnn.Linear(in_features=in_features, out_features=11),\n","\t\t\t\t#nn.Softmax()\n","\t\t)\n","\n","\tdef forward(self, x):\n","\t\tx = self.head_block(x)\n","\t\treturn x\n","\n","################################################################################\n","################################### NETWORKS ###################################\n","################################################################################\n","class Conv1Layer(nn.Module):\n","\n","\tdef __init__(self, single_sample, channels=[8],\n","\t\t\t\t\t\t\t conv_kernel_sizes=[3],\n","\t\t\t\t\t\t\t conv_strides=[1],\n","\t\t\t\t\t\t\t conv_paddings=[0],\n","\t\t\t\t\t\t\t pool_masks=[True],\n","\t\t\t\t\t\t\t pool_kernel_sizes=[2],\n","\t\t\t\t\t\t\t pool_strides=[2],\n","\t\t\t\t\t\t\t linear_features=[128, 64],\n","\t\t\t\t\t\t\t dropout_probs=[0, 0]):\n","\n","\t\t\"\"\"Convolutional neural network with 1 conv layer and 3 linear layers.\n","\t\tAll hyperparams are flexible and initialized using lists (or array-likes).\n","\t\tThe nth entry in each list corrosponds to the nth layer\n","\n","\t\t:param single_sample: a sample mfcc to run through the network on init to \n","\t\tget layer sizes\n","\t\t:param channels:\n","\t\t:param conv_kernel_sizes:\n","\t\t:param conv_paddings:\n","\t\t:param pool_masks: array of booleans to control max pooling\n","\t\t\tex: [False, True] means no max pooling after 1st layer, but max pooling \n","\t\t\tafter second layer. Other hyperparams for maxpooling must be passed so\n","\t\t\tthat alignment is consistent. ex: in the [False, True] example, one could\n","\t\t\tpass [3, 2] for pool kernel size. The 3 does nothing but the 2 will use \n","\t\t\ta pool kernel size of 2. Passing only [2] will result in an error even if \n","\t\t\tthere is only one maxpool layer.\n","\t\t:param pool_kernel_sizes:\n","\t\t:param pool_strides:\n","\t\t:param linear features: output sizes for linear layers (input size\n","\t\t\tdetermined on init by one_mfcc)\n","\t\t:param dropout_probs:\n","\n","\t\t\"\"\"\n","\t\tsuper(Conv1Layer, self).__init__()\n","\n","\t\t# convolutional blocks\n","\t\tself.conv1 = ConvBlock(in_channels=1, out_channels=channels[0],\n","\t\t\t\t\t\t\t\t\t\t\t\t\t conv_kernel_size=conv_kernel_sizes[0],\n","\t\t\t\t\t\t\t\t\t\t\t\t\t conv_stride=conv_strides[0],\n","\t\t\t\t\t\t\t\t\t\t\t\t\t conv_padding=conv_paddings[0],\n","\t\t\t\t\t\t\t\t\t\t\t\t\t inc_pool=pool_masks[0],\n","\t\t\t\t\t\t\t\t\t\t\t\t\t pool_kernel_size=pool_kernel_sizes[0],\n","\t\t\t\t\t\t\t\t\t\t\t\t\t pool_stride=pool_strides[0])\n","\t\n","\t\t# run a single sample through the convolutional block to get output size\n","\t\t# https://discuss.pytorch.org/t/convolution-and-pooling-layers-need-a-method-to-calculate-output-size/21895\n","\t\tsample_output1 = self.conv1(torch.from_numpy(\n","\t\t\t\tsingle_sample[np.newaxis,...].astype(np.float32)))\n","\t\n","\t\tsample_flattened = sample_output1.flatten(start_dim=1)\n"," \n","\t\t# linear blocks\n","\t\tself.linear1 = LinearBlock(in_features=(sample_flattened.shape[1]),\n","\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tout_features=(linear_features[0]),\n","\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tdropout_prob=dropout_probs[0])\n","\t\tself.linear2 = LinearBlock(in_features=(linear_features[0]),\n","\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tout_features=(linear_features[1]),\n","\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tdropout_prob=dropout_probs[1])\n","\t\tself.head = HeadBlock(in_features=(linear_features[1]))\n","\n","\tdef forward(self, x):\n","\t\tx = self.conv1(x)\n","\t\tx = x.flatten(start_dim=1)\n","\t\tx = self.linear1(x)\n","\t\tx = self.linear2(x)\n","\t\tx = self.head(x)\n","\t\treturn x\n","\n","class Conv3Layer(nn.Module):\n","\tdef __init__(self, single_sample, channels=[8, 16, 32],\n","\t\t\t\t\t\t\t conv_kernel_sizes=[3, 3, 3],\n","\t\t\t\t\t\t\t conv_strides=[1, 1, 1],\n","\t\t\t\t\t\t\t conv_paddings=[0, 0, 1,],\n","\t\t\t\t\t\t\t pool_masks=[True, False, False],\n","\t\t\t\t\t\t\t pool_kernel_sizes=[2, 2, 2],\n","\t\t\t\t\t\t\t pool_strides=[2, 2, 2],\n","\t\t\t\t\t\t\t linear_features=[128, 64],\n","\t\t\t\t\t\t\t dropout_probs=[0, 0]):\n","\n","\t\t\"\"\"Convolutional neural network with 3 conv layers and 3 linear layers.\n","\t\tAll hyperparams are flexible and initialized using lists (or array-likes).\n","\t\tThe nth entry in each list corrosponds to the nth layer\n","\n","\t\t:param single_sample: a sample mfcc to run through the network on init to \n","\t\tget layer sizes\n","\t\t:param channels:\n","\t\t:param conv_kernel_sizes:\n","\t\t:param conv_paddings:\n","\t\t:param pool_masks: array of booleans to control max pooling\n","\t\t\tex: [False, True] means no max pooling after 1st layer, but max pooling \n","\t\t\tafter second layer. Other hyperparams for maxpooling must be passed so\n","\t\t\tthat alignment is consistent. ex: in the [False, True] example, one could\n","\t\t\tpass [3, 2] for pool kernel size. The 3 does nothing but the 2 will use \n","\t\t\ta pool kernel size of 2. Passing only [2] will result in an error even if \n","\t\t\tthere is only one maxpool layer.\n","\t\t:param pool_kernel_sizes:\n","\t\t:param pool_strides:\n","\t\t:param linear features: output sizes for linear layers (input size\n","\t\t\tdetermined on init by one_mfcc)\n","\t\t:param dropout_probs:\n","\t\t\"\"\"\n","\n","\t\tsuper(Conv3Layer, self).__init__()\n","\n","\t\tself.conv1 = ConvBlock(in_channels=1, out_channels=channels[0],\n","\t\t\t\t\t\t\t\t\t\t\t\t\t conv_kernel_size=conv_kernel_sizes[0],\n","\t\t\t\t\t\t\t\t\t\t\t\t\t conv_stride=conv_strides[0],\n","\t\t\t\t\t\t\t\t\t\t\t\t\t conv_padding=conv_paddings[0],\n","\t\t\t\t\t\t\t\t\t\t\t\t\t inc_pool=pool_masks[0],\n","\t\t\t\t\t\t\t\t\t\t\t\t\t pool_kernel_size=pool_kernel_sizes[0],\n","\t\t\t\t\t\t\t\t\t\t\t\t\t pool_stride=pool_strides[0])\n","\t\t\n","\t\tself.conv2 = ConvBlock(in_channels=channels[0], out_channels=channels[1],\n","\t\t\t\t\t\t\t\t\t\t\t\t\t conv_kernel_size=conv_kernel_sizes[1],\n","\t\t\t\t\t\t\t\t\t\t\t\t\t conv_stride=conv_strides[1],\n","\t\t\t\t\t\t\t\t\t\t\t\t\t conv_padding=conv_paddings[1],\n","\t\t\t\t\t\t\t\t\t\t\t\t\t inc_pool=pool_masks[1],\n","\t\t\t\t\t\t\t\t\t\t\t\t\t pool_kernel_size=pool_kernel_sizes[1],\n","\t\t\t\t\t\t\t\t\t\t\t\t\t pool_stride=pool_strides[1])\n","\n","\t\tself.conv3 = ConvBlock(in_channels=channels[1], out_channels=channels[2],\n","\t\t\t\t\t\t\t\t\t\t\t\t\t conv_kernel_size=conv_kernel_sizes[2],\n","\t\t\t\t\t\t\t\t\t\t\t\t\t conv_stride=conv_strides[2],\n","\t\t\t\t\t\t\t\t\t\t\t\t\t conv_padding=conv_paddings[2],\n","\t\t\t\t\t\t\t\t\t\t\t\t\t inc_pool=pool_masks[2],\n","\t\t\t\t\t\t\t\t\t\t\t\t\t pool_kernel_size=pool_kernel_sizes[2],\n","\t\t\t\t\t\t\t\t\t\t\t\t\t pool_stride=pool_strides[2])\n","\t\t\n","\t\t# calculate size for linear layers\n","\t\tsample_output1 = self.conv1(torch.from_numpy(\n","\t\t\t\tsingle_sample[np.newaxis,...].astype(np.float32)))\n","\t\tsample_output2 = self.conv2(sample_output1)\n","\t\tsample_output3 = self.conv3(sample_output2)\n","\t\tsample_flattened = sample_output3.flatten(start_dim=1)\n","\n","\t\t# linear blocks\n","\t\tself.linear1 = LinearBlock(in_features=(sample_flattened.shape[1]),\n","\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tout_features=(linear_features[0]),\n","\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tdropout_prob=dropout_probs[0])\n","\t\tself.linear2 = LinearBlock(in_features=(linear_features[0]),\n","\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tout_features=(linear_features[1]),\n","\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tdropout_prob=dropout_probs[1])\n","\t\tself.head = HeadBlock(in_features=(linear_features[1]))\n","\t\n","\tdef forward(self, x):\n","\t\tx = self.conv1(x)\n","\t\tx = self.conv2(x)\n","\t\tx = self.conv3(x)\n","\t\tx = x.flatten(start_dim=1)\n","\t\tx = self.linear1(x)\n","\t\tx = self.linear2(x)\n","\t\tx = self.head(x)\n","\t\treturn x\n","\n","class Conv5Layer(nn.Module):\n","\tdef __init__(self, single_sample, channels= [8, 8, 32, 32, 64],\n","\t\t\t\t\t\t\t conv_kernel_sizes=[3, 3, 3, 3, 3],\n","\t\t\t\t\t\t\t conv_strides=[1, 1, 1, 1, 1],\n","\t\t\t\t\t\t\t conv_paddings=[0, 0, 1, 1, 1],\n","\t\t\t\t\t\t\t pool_masks=[True, False, False, False, False],\n","\t\t\t\t\t\t\t pool_kernel_sizes=[2, 2, 2, 2, 2],\n","\t\t\t\t\t\t\t pool_strides=[2, 2, 2, 2, 2],\n","\t\t\t\t\t\t\t linear_features=[128, 64],\n","\t\t\t\t\t\t\t dropout_probs=[0, 0]):\n","\t\t\n","\t\t\"\"\"Convolutional neural network with 3 conv layers and 3 linear layers.\n","\t\tAll hyperparams are flexible and initialized using lists (or array-likes).\n","\t\tThe nth entry in each list corrosponds to the nth layer\n","\n","\t\t:param single_sample: a sample mfcc to run through the network on init to \n","\t\tget layer sizes\n","\t\t:param channels:\n","\t\t:param conv_kernel_sizes:\n","\t\t:param conv_paddings:\n","\t\t:param pool_masks: array of booleans to control max pooling\n","\t\t\tex: [False, True] means no max pooling after 1st layer, but max pooling \n","\t\t\tafter second layer. Other hyperparams for maxpooling must be passed so\n","\t\t\tthat alignment is consistent. ex: in the [False, True] example, one could\n","\t\t\tpass [3, 2] for pool kernel size. The 3 does nothing but the 2 will use \n","\t\t\ta pool kernel size of 2. Passing only [2] will result in an error even if \n","\t\t\tthere is only one maxpool layer.\n","\t\t:param pool_kernel_sizes:\n","\t\t:param pool_strides:\n","\t\t:param linear features: output sizes for linear layers (input size\n","\t\t\tdetermined on init by one_mfcc)\n","\t\t:param dropout_probs:\n","\t\t\"\"\"\n","\n","\t\tsuper(Conv5Layer, self).__init__()\n","\n","\t\t# convolutional layers\n","\t\tself.conv1 = ConvBlock(in_channels=1, out_channels=channels[0],\n","\t\t\t\t\t\t\t\t\t\t\t\t\t conv_kernel_size=conv_kernel_sizes[0],\n","\t\t\t\t\t\t\t\t\t\t\t\t\t conv_stride=conv_strides[0],\n","\t\t\t\t\t\t\t\t\t\t\t\t\t conv_padding=conv_paddings[0],\n","\t\t\t\t\t\t\t\t\t\t\t\t\t inc_pool=pool_masks[0],\n","\t\t\t\t\t\t\t\t\t\t\t\t\t pool_kernel_size=pool_kernel_sizes[0],\n","\t\t\t\t\t\t\t\t\t\t\t\t\t pool_stride=pool_strides[0])\n","\t\t\n","\t\tself.conv2 = ConvBlock(in_channels=channels[0], out_channels=channels[1],\n","\t\t\t\t\t\t\t\t\t\t\t\t\t conv_kernel_size=conv_kernel_sizes[1],\n","\t\t\t\t\t\t\t\t\t\t\t\t\t conv_stride=conv_strides[1],\n","\t\t\t\t\t\t\t\t\t\t\t\t\t conv_padding=conv_paddings[1],\n","\t\t\t\t\t\t\t\t\t\t\t\t\t inc_pool=pool_masks[1],\n","\t\t\t\t\t\t\t\t\t\t\t\t\t pool_kernel_size=pool_kernel_sizes[1],\n","\t\t\t\t\t\t\t\t\t\t\t\t\t pool_stride=pool_strides[1])\n","\n","\t\tself.conv3 = ConvBlock(in_channels=channels[1], out_channels=channels[2],\n","\t\t\t\t\t\t\t\t\t\t\t\t\t conv_kernel_size=conv_kernel_sizes[2],\n","\t\t\t\t\t\t\t\t\t\t\t\t\t conv_stride=conv_strides[2],\n","\t\t\t\t\t\t\t\t\t\t\t\t\t conv_padding=conv_paddings[2],\n","\t\t\t\t\t\t\t\t\t\t\t\t\t inc_pool=pool_masks[2],\n","\t\t\t\t\t\t\t\t\t\t\t\t\t pool_kernel_size=pool_kernel_sizes[2],\n","\t\t\t\t\t\t\t\t\t\t\t\t\t pool_stride=pool_strides[2])\n","\t\t\n","\t\tself.conv4 = ConvBlock(in_channels=channels[2], out_channels=channels[3],\n","\t\t\t\t\t\t\t\t\t\t\t\t\t conv_kernel_size=conv_kernel_sizes[3],\n","\t\t\t\t\t\t\t\t\t\t\t\t\t conv_stride=conv_strides[3],\n","\t\t\t\t\t\t\t\t\t\t\t\t\t conv_padding=conv_paddings[3],\n","\t\t\t\t\t\t\t\t\t\t\t\t\t inc_pool=pool_masks[3],\n","\t\t\t\t\t\t\t\t\t\t\t\t\t pool_kernel_size=pool_kernel_sizes[3],\n","\t\t\t\t\t\t\t\t\t\t\t\t\t pool_stride=pool_strides[3])\n","\t\t\n","\t\tself.conv5 = ConvBlock(in_channels=channels[3], out_channels=channels[4],\n","\t\t\t\t\t\t\t\t\t\t\t\t\t conv_kernel_size=conv_kernel_sizes[4],\n","\t\t\t\t\t\t\t\t\t\t\t\t\t conv_stride=conv_strides[4],\n","\t\t\t\t\t\t\t\t\t\t\t\t\t conv_padding=conv_paddings[4],\n","\t\t\t\t\t\t\t\t\t\t\t\t\t inc_pool=pool_masks[4],\n","\t\t\t\t\t\t\t\t\t\t\t\t\t pool_kernel_size=pool_kernel_sizes[4],\n","\t\t\t\t\t\t\t\t\t\t\t\t\t pool_stride=pool_strides[4])\n","\t\t\n","\t\t# calculate size for linear layers\n","\t\tsample_output1 = self.conv1(torch.from_numpy(\n","\t\t\t\tsingle_sample[np.newaxis,...].astype(np.float32)))\n","\t\tsample_output2 = self.conv2(sample_output1)\n","\t\tsample_output3 = self.conv3(sample_output2)\n","\t\tsample_output4 = self.conv4(sample_output3)\n","\t\tsample_output5 = self.conv5(sample_output4)\n","\t\tsample_flattened = sample_output5.flatten(start_dim=1)\n","\n","\n","\t\t# linear blocks\n","\t\tself.linear1 = LinearBlock(in_features=(sample_flattened.shape[1]),\n","\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tout_features=(linear_features[0]),\n","\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tdropout_prob=dropout_probs[0])\n","\t\tself.linear2 = LinearBlock(in_features=(linear_features[0]),\n","\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tout_features=(linear_features[1]),\n","\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tdropout_prob=dropout_probs[1])\n","\t\tself.head = HeadBlock(in_features=(linear_features[1]))\n","\t\n","\tdef forward(self, x):\n","\t\tx = self.conv1(x)\n","\t\tx = self.conv2(x)\n","\t\tx = self.conv3(x)\n","\t\tx = self.conv4(x)\n","\t\tx = self.conv5(x)\n","\t\tx = x.flatten(start_dim=1)\n","\t\tx = self.linear1(x)\n","\t\tx = self.linear2(x)\n","\t\tx = self.head(x)\n","\t\treturn x\n","\n","### This one doesn't seem to be working yet\n","class ConvNLayer(nn.Module):\n","\tdef __init__(self, single_sample, \n","\n","\t\tnum_conv_layers=2,\n","\t\tchannels=[8, 16],\n","\t\tconv_kernel_sizes=[3, 3],\n","\t\tconv_strides=[1, 1],\n","\t\tconv_paddings=[1, 1],\n","\t\tpool_masks=[True, True],\n","\t\tpool_kernel_sizes=[2, 2],\n","\t\tpool_strides=[2, 2],\n","\t\t\n","\t\tnum_linear_layers=2,\n","\t\tlinear_features=[128, 64],\n","\t\tdropout_probs=[0, 0]\n","\t\t):\n","\t\t\"\"\"Convolutional neural net with an arbitrary number of convolutional layers\n","\t\t\"\"\"\n","\t\tsuper(ConvNLayer, self).__init__()\n","\n","\t\tself.num_conv_layers = num_conv_layers\n","\t\tself.num_linear_layers = num_linear_layers\n","\n","\t\t# prepend 1 to input channels since there is only one\n","\t\tchannels.insert(0, 1)\n","\n","\t\t# define list of convolutional layers\n","\t\tself.conv_layers = [\n","\t\t\tConvBlock(\n","\t\t\t\tin_channels = channels[i],\n","\t\t\t\tout_channels = channels[i+1],\n","\t\t\t\tconv_kernel_size = conv_kernel_sizes[i],\n","\t\t\t\tconv_stride = conv_strides[i],\n","\t\t\t\tconv_padding = conv_paddings[i],\n","\t\t\t\tinc_pool = pool_masks[i],\n","\t\t\t\tpool_kernel_size = pool_kernel_sizes[i],\n","\t\t\t\tpool_stride = pool_strides[i])\n","\t\tfor i in range(self.num_conv_layers)]\n","\n","\t\t# calculate size of linear layers\n","\t\tsample = torch.from_numpy(\n","\t\t\tsingle_sample[np.newaxis,...].astype(np.float32)\n","\t\t)\n","\n","\t\tfor i in range(self.num_conv_layers):\n","\t\t\tsample = self.conv_layers[i](sample)\n","\n","\t\tsample_flattened = sample.flatten(start_dim=1)\n","\n","\t\t# prepend shape of input to linear block\n","\t\tlinear_features.insert(0, sample_flattened.shape[1])\n","\n","\t\t# define list of linear layers\n","\t\tself.linear_layers = [\n","\t\t\tLinearBlock(\n","\t\t\t\tin_features = (linear_features[i]),\n","\t\t\t\tout_features = (linear_features[i+1]),\n","\t\t\t\tdropout_prob = dropout_probs[i])\n","\t\t\tfor i in range(self.num_linear_layers)\n","\t\t]\n","\n","\t\t# define output head\n","\t\tself.head = HeadBlock(in_features=(linear_features[-1]))\n","\n","\tdef forward(self, x):\n","\t\tfor i in range(self.num_conv_layers):\n","\t\t\tprint(self.conv_layers[i])\n","\t\t\tx = self.conv_layers[i](x)\n","\t\t\n","\t\tx = x.flatten(start_dim=1)\n","\n","\t\tfor i in range(self.num_linear_layers):\n","\t\t\tx = self.linear_layers[i](x)\n","\t\t\n","\t\tx = self.head(x)\n","\t\treturn x\n","\n","models_dict = {\n","\t\"Conv_1_layer\": Conv1Layer,\n","\t\"Conv_3_layer\": Conv3Layer,\n","\t\"Conv_5_layer\": Conv5Layer,\n","\t\"Conv_N_layer\": ConvNLayer,\n","}\n","\n","models_dict"],"execution_count":4,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'Conv_1_layer': __main__.Conv1Layer,\n"," 'Conv_3_layer': __main__.Conv3Layer,\n"," 'Conv_5_layer': __main__.Conv5Layer,\n"," 'Conv_N_layer': __main__.ConvNLayer}"]},"metadata":{"tags":[]},"execution_count":4}]},{"cell_type":"markdown","metadata":{"id":"pjr3RAfVyfgB"},"source":["# Get a sample & test"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bp5qY0JmyWee","executionInfo":{"status":"ok","timestamp":1619530993359,"user_tz":240,"elapsed":29547,"user":{"displayName":"Alex Palermo","photoUrl":"","userId":"06488501085160491296"}},"outputId":"18d02b2c-a8c0-4cda-dbe8-b9919bc15392"},"source":["train_loader, val_loader, dataset = prep_dataset(\n","      filename=filename, batch_size=5, val_split=0.2)\n","single_sample = dataset[0]\n","\n","one_mfcc = np.array(single_sample['mfccs'])\n","one_mfcc.shape"],"execution_count":5,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(1, 13, 517)"]},"metadata":{"tags":[]},"execution_count":5}]},{"cell_type":"markdown","metadata":{"id":"LcemxF9NymoP"},"source":["# Train Loop"]},{"cell_type":"code","metadata":{"id":"5av__3mSyj9_","executionInfo":{"status":"ok","timestamp":1619530995285,"user_tz":240,"elapsed":1055,"user":{"displayName":"Alex Palermo","photoUrl":"","userId":"06488501085160491296"}}},"source":["def train_model(filename=\"irmas_data_mfcc13_hop_length256_n_fft2048\", model_id=\"TestModel\",\n","                num_epochs=2, interval=16, lr=0.001, batch_size=64,\n","                val_split=0.2, save_checkpoint=False, checkpoint_path=\"\",\n","                notes=\"\", checkpoint_name=\"utitled.pt\", criterion=torch.nn.NLLLoss(),\n","                patience=None, min_epochs=5, buffer=0.05, dropout_prob=None,\n","                model_args={}, experiment_params={}):\n","  \"\"\"Model training loop for music analysis project. Currently, this loop only supports\n","  models that take input in the shape [mini_batch, channels, L, W].\n","\n","  :param filename:\n","  :param model_id:\n","  :param num_epochs:\n","  :param interval:\n","  :param lr:\n","  :param batch_size:\n","  :param val_split:\n","  :param save_checkpoint:\n","  :param checkpoint_path:\n","  :param notes:\n","  :param checkpoint name:\n","  :param criterion:\n","  :param patience: If validation loss does not improve over this many epochs, stop training\n","  \"\"\"\n","\n","  # Initialize device\n","  device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","  print(\"device: \", device)\n","  \n","  # get train and validation set, print metadata\n","  train_loader, val_loader, dataset = prep_dataset(\n","      filename=filename, batch_size=batch_size, val_split=val_split)\n","\n","  print(\"dataset metadata: \", dataset.metadata)\n","\n","  # param for early stopping\n","  stop_next = False\n","\n","  # get number of train and validdation samples\n","  train_samples = round(len(dataset) * (1-val_split))\n","  val_samples = round(len(dataset)*val_split)\n","\n","  # initialize loss history and accuracy history for each epoch\n","  # this is the stored history for the train and validation metrics\n","  epoch_hist = []\n","  avg_train_loss_hist = []  # training loss for each epoch\n","  std_train_loss_hist = []\n","  avg_val_loss_hist = []    # validation loss for each epoch\n","  std_val_loss_hist = []\n","  train_acc_hist = []       # training accuracy for each epoch\n","  train_prec_hist = []\n","  train_recall_hist = []\n","  train_f1_hist = []\n","  val_acc_hist = []         # validation accuracy for each epoch\n","  val_prec_hist = []\n","  val_recall_hist = []\n","  val_f1_hist = []\n","\n","\n","  # get one sample to load initial shape for neural net\n","  single_sample = dataset[0]\n","  one_mfcc = np.array(single_sample['mfccs'])\n","  print(\"train model: data loaders initialized\")\n","  print(\"sample shape = \", one_mfcc.shape)\n","\n","  # initialize model\n","  #model = models_dict[model_id](one_mfcc, dropout_prob).to(device)\n","  model = models_dict[model_id](one_mfcc, **model_args).to(device)\n","  print(\"model loaded\")\n","  summary_str = str(summary(model, one_mfcc.shape, verbose=0))\n","\n","  print(summary_str)\n","\n","  # initialize optimizer and criterion\n","  optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n","  print(\"criterion: \", criterion)\n","\n","  n_train_steps = len(train_loader)\n","  n_val_steps = len(val_loader)\n","\n","  ### loop epochs\n","  for epoch in range(num_epochs):\n","    print(\"\\n\\ntraining epoch: \", epoch)\n","    epoch_hist.append(epoch+1)\n","    epoch_time_start = time.time()\n","    interval_time_start = time.time()\n","    model.to(device)\n","\n","    # at the start of the epoch, set all tracked params to zero\n","    train_losses = []\n","    val_losses = []\n","    inter_epoch_loss = []\n","    train_num_correct = 0\n","    val_num_correct = 0\n","\n","    # set params to be tracked within the epoch (\"inter-epoch\")\n","    # these will be outputted at each interval, but not saved\n","    inter_epoch_num_correct = 0\n","\n","    ### Training loop\n","    model.train()\n","    print(\"model set to train\")\n","    train_preds = []\n","    train_targets = []\n","    for i, sample in enumerate(train_loader):\n","\n","      # prep input and target tensor\n","      input_tensor = torch.from_numpy(\n","          np.array(sample['mfccs']).astype(np.float32)).to(device)\n","      targets = sample['instrument']\n","      target_tensor = torch.squeeze(torch.tensor(targets), dim=1)\n","      #print(\"target tensor after processing: \", target_tensor)\n","      train_targets.extend(list(targets.numpy()))\n","      # make predictions\n","      try:\n","        predictions = torch.squeeze(model(input_tensor).to('cpu'), dim=1)\n","      except Exception as e:\n","        print(\"EXCEPTION THROWN: \", e)\n","      # compute loss and do back-propagation\n","      loss = criterion(predictions, target_tensor)\n","      optimizer.zero_grad()\n","      loss.backward()\n","      optimizer.step()\n","\n","      # append the loss to overall \n","      train_losses.append(loss.item())\n","      inter_epoch_loss.append(loss.item())\n","\n","      # compute accuracies\n","      with torch.no_grad():\n","        predictions_arr = predictions.numpy()\n","        preds = [np.argmax(predictions_arr[i]) \n","          for i in range(len(target_tensor))]\n","        # inter-epoch accuracy (reset this at each interval)\n","        inter_epoch_num_correct += np.sum([target_tensor[i] == np.argmax(predictions[i])\n","          for i in range(len(target_tensor))])\n","        \n","        # epoch accuracy (this is tracked and saved)\n","        train_num_correct += np.sum([target_tensor[i] == np.argmax(predictions[i])\n","          for i in range(len(target_tensor))])\n","        #print(\"debugging in epoch: preds = \", preds)\n","        train_preds.extend(preds)\n","\n","      # print step info\n","      if i % interval == 0:\n","\n","        # time elapsed\n","        interval_time_end = time.time()\n","\n","        # compute mean and std of losses\n","        inter_epoch_loss_avg = np.mean(inter_epoch_loss)\n","        inter_epoch_loss_std = np.std(inter_epoch_loss)\n","        \n","        # compute inter-epoch accuracy\n","        # note, this accuracy may be incorrect at the end of each epoch\n","        # when the batch size is slightly different\n","        acc = inter_epoch_num_correct / (interval*batch_size)\n","        print(f\"Epoch [{epoch+1}/{num_epochs}], step [{i+1}/{n_train_steps}], \",\n","              f\"Loss: {inter_epoch_loss_avg:.4f} +/- {inter_epoch_loss_std:.4f}, \",\n","              f\"accuracy: {acc}, \"\n","              f\"time elapsed = {interval_time_end-interval_time_start}s\")\n","        interval_time_start = time.time()\n","\n","        # reset inter_epoch metrics\n","        inter_epoch_num_correct = 0\n","        inter_epoch_loss = []\n","\n","    ### training loop finished\n","    # append the accuracy\n","    train_acc_hist.append(train_num_correct / train_samples)\n","\n","    # calculate classification metrics\n","    train_targets = np.array(train_targets).ravel()\n","    train_preds = np.array(train_preds).ravel()\n","    # print(\"debugging: train targets: \", train_targets)\n","    # print(\"debugging: train predictions: \", train_preds)\n","    train_prec, train_recall, train_f1, _ = precision_recall_fscore_support(train_targets, train_preds,\n","                                                      average='micro')\n","\n","    ### Validation loop\n","    model.eval()\n","    print(\"model set to eval\")\n","    val_preds = []\n","    val_targets = []\n","    with torch.no_grad():\n","\n","      num_correct = 0\n","      for i, sample in enumerate(val_loader):\n","        \n","        # prep input and target tensor\n","        input_tensor = torch.from_numpy(\n","            np.array(sample['mfccs']).astype(np.float32)).to(device)\n","        targets = sample['instrument']\n","        val_targets.extend(list(targets.numpy()))\n","        target_tensor = torch.squeeze(torch.tensor(targets), dim=1)\n","        #target_tensor = torch.squeeze(torch.tensor(sample['instrument']), dim=1)\n","\n","        # make predictions\n","        try:\n","          predictions = torch.squeeze(model(input_tensor).to('cpu'), dim=1)\n","        except Exception as e:\n","          print(\"EXCEPTION THROWN: \", e)\n","        # compute and append losses\n","        loss = criterion(predictions, target_tensor)\n","        val_losses.append(loss.item())\n","\n","        predictions_arr = predictions.numpy()\n","        preds = [np.argmax(predictions_arr[i]) \n","          for i in range(len(target_tensor))]\n","        val_preds.extend(preds)\n","        # get num correct to comput accuracy\n","        val_num_correct += np.sum([target_tensor[i] == np.argmax(predictions[i])\n","          for i in range(len(target_tensor))])\n","      \n","      ### validation loop finished. prep model and metrics for saving\n","      # calculate validation accuracy\n","      val_acc_hist.append(val_num_correct / val_samples)\n","      val_targets = np.array(val_targets).ravel()\n","      val_preds = np.array(val_preds).ravel()\n","      # print(\"debugging: train targets: \", train_targets)\n","      # print(\"debugging: train predictions: \", train_preds)\n","      val_prec, val_recall, val_f1, _ = precision_recall_fscore_support(val_targets, val_preds,\n","                                                      average='micro')\n","      # calculate mean and standard deviation of losses\n","      avg_train_loss = np.mean(train_losses)\n","      std_train_loss = np.std(train_losses)\n","      avg_val_loss = np.mean(val_losses)\n","      std_val_loss = np.std(val_losses)\n","\n","      # append mean and standard deviation to histories\n","      avg_train_loss_hist.append(avg_train_loss) \n","      std_train_loss_hist.append(std_train_loss)  \n","      avg_val_loss_hist.append(avg_val_loss)\n","      std_val_loss_hist.append(std_val_loss)\n","\n","      train_prec_hist.append(train_prec)\n","      train_recall_hist.append(train_recall)\n","      train_f1_hist.append(train_f1)\n","\n","      val_prec_hist.append(val_prec)\n","      val_recall_hist.append(val_recall)\n","      val_f1_hist.append(val_f1)\n","    \n","    ### epoch training finished, output results and save checkpoint\n","\n","    # text output\n","    epoch_time_end = time.time()\n","    print(f\"\\nEPOCH FINISHED: , \",\n","          f\"training: acc = {train_acc_hist[-1]:.3f}, \",\n","          f\"precision = {train_prec_hist[-1]:.3f}\",\n","          f\"recall = {train_recall_hist[-1]:.3f}\",\n","          f\"f1 = {train_f1_hist[-1]:.3f}\",\n","          f\"::: val: acc = {val_acc_hist[-1]:.3f}, \",\n","          f\"precision = {val_prec_hist[-1]:.3f}\",\n","          f\"recall = {val_recall_hist[-1]:.3f}\",\n","          f\"time elapsed = {epoch_time_end-epoch_time_start}s\")\n","    \n","    # make a plot\n","    plt.close(\"all\")\n","    fig, ax = plt.subplots(ncols=2, figsize=[15, 5])\n","    #ax.scatter(epoch_hist, avg_train_loss_hist, c='r', label=\"train loss\", )\n","    ax[0].plot(epoch_hist, avg_train_loss_hist, 'ro--', label=\"train loss\", )\n","    ax[0].errorbar(x=epoch_hist, y=avg_train_loss_hist, yerr=std_train_loss_hist,\n","                capsize=5, ls='none', color='r')\n","\n","    # ax.scatter(epoch_hist, avg_val_loss_hist, c='b', label=\"val loss\", )\n","    ax[0].plot(epoch_hist, avg_val_loss_hist, 'ko--', label=\"val loss\", )\n","    ax[0].errorbar(x=epoch_hist, y=avg_val_loss_hist, yerr=std_val_loss_hist,\n","                capsize=5, ls='none', color='k')\n","    \n","    ax[0].set_xlabel(\"epoch\")\n","    ax[0].set_ylabel(\"loss\")\n","    ax[0].legend()\n","    \n","\n","    ax[1].plot(epoch_hist, train_acc_hist, 'r-.', label=\"train accuracy\", \n","                  marker='s')\n","\n","    ax[1].plot(epoch_hist, val_acc_hist, 'k-.', label=\"val accuracy\",\n","                  marker='s')\n","    ax[1].set_ylabel(\"accuracy\")\n","    ax[1].set_xlabel(\"epoch\")\n","    ax[1].set_ylim([0, 1])\n","    ax[1].legend()\n","    fig.tight_layout(pad=1)\n","    plt.show(block=False)\n","\n","    # check validation loss if we need to stop training\n","    # print(\"validation loss hist: \", avg_val_loss_hist)\n","    # if (epoch > patience) and all(avg_val_loss_hist[-1-i] >= avg_val_loss_hist[-1-i-1]\n","    #                               for i in range(patience)):\n","    model.to('cpu')\n","    if (epoch > min_epochs) and (\n","        #avg_val_loss_hist[-1] > (std_val_loss_hist[-1] + std_train_loss_hist[-1] + avg_train_loss_hist[-1] + buffer)):\n","        (avg_val_loss_hist[-1] - avg_train_loss_hist[-1] + buffer) > (std_train_loss_hist[-1] + std_val_loss_hist[-1])):\n","        #avg_val_loss_hist[-1] > (std_val_loss_hist[-1] + std_train_loss_hist[-1] + buffer)):\n","                    # and any(avg_val_loss_hist[-1-i] >= avg_val_loss_hist[-1-i-1]\n","                    #                                   for i in range(patience)):\n","      # save model\n","      # TODO: refactor this so torch.save isn't repeated\n","      if save_checkpoint:\n","        notes = notes + \"\\n\\n stopped early\"\n","        torch.save({\n","            'filename': filename,\n","            'epochs': epoch_hist,\n","            'model_id': model_id,\n","            'model_state_dict': model.state_dict(),\n","            'model_args': model_args,\n","            'metrics':{\n","              'avg_train_loss_hist': avg_train_loss_hist,\n","              'std_train_loss_hist': std_train_loss_hist,\n","              'avg_val_loss_hist': avg_val_loss_hist,\n","              'std_val_loss_hist': std_val_loss_hist,\n","              'train_acc_hist': train_acc_hist,\n","              'train_prec_hist': train_prec_hist,\n","              'train_recall_hist': train_recall_hist,\n","              'train_f1_hist': train_f1_hist,\n","              'val_acc_hist': val_acc_hist,\n","              'val_prec_hist': val_prec_hist,\n","              'val_recall_hist': val_recall_hist,\n","              'val_f1_hist': val_f1_hist,},\n","            'dataset_info': dataset.metadata,\n","            'notes': notes,\n","            'summary': summary_str,\n","            'experiment_params': experiment_params,\n","        }, checkpoint_path+checkpoint_name)\n","        print(\"model saved\")\n","\n","      if stop_next:\n","        print(\"stopping early\")\n","        break\n","      else:\n","        stop_next = True\n","    else:\n","      stop_next = False\n","\n","    # save model\n","    if save_checkpoint:\n","      torch.save({\n","            'filename': filename,\n","            'epochs': epoch_hist,\n","            'model_id': model_id,\n","            'model_state_dict': model.state_dict(),\n","            'model_args': model_args,\n","            'metrics':{\n","              'avg_train_loss_hist': avg_train_loss_hist,\n","              'std_train_loss_hist': std_train_loss_hist,\n","              'avg_val_loss_hist': avg_val_loss_hist,\n","              'std_val_loss_hist': std_val_loss_hist,\n","              'train_acc_hist': train_acc_hist,\n","              'train_prec_hist': train_prec_hist,\n","              'train_recall_hist': train_recall_hist,\n","              'train_f1_hist': train_f1_hist,\n","              'val_acc_hist': val_acc_hist,\n","              'val_prec_hist': val_prec_hist,\n","              'val_recall_hist': val_recall_hist,\n","              'val_f1_hist': val_f1_hist,},\n","            'dataset_info': dataset.metadata,\n","            'notes': notes,\n","            'summary': summary_str,\n","            'experiment_params': experiment_params,\n","        }, checkpoint_path+checkpoint_name)\n","      print(\"model saved\")"],"execution_count":6,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2-YWf2lDzASm"},"source":["# Initialize params"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"I2mXe_BTy5qg","executionInfo":{"status":"error","timestamp":1619531033911,"user_tz":240,"elapsed":29892,"user":{"displayName":"Alex Palermo","photoUrl":"","userId":"06488501085160491296"}},"outputId":"c3e62427-97a1-4e42-84a5-552703d8ea43"},"source":["CHECKPOINT_ROOT = \"/content/drive/MyDrive/ITCS 5156 project/trained_models/batch_size/\"\n","\n","model_args = {\n","\t\"channels\": [8, 8, 32, 32, 64],\n","\t\"conv_kernel_sizes\": [3, 3, 3, 3, 3],\n","\t\"conv_strides\": [1, 1, 1, 1, 1],\n","\t\"conv_paddings\": [1, 1, 1, 1, 1],\n","\t\"pool_masks\": [True, True, True, True, True],\n","\t\"pool_kernel_sizes\": [2, 2, 2, (1, 2), (1, 2)],\n","\t\"pool_strides\": [2, 2, 2, (1, 2), (1, 2)],\n","\t\"linear_features\": [128, 64],\n","\t\"dropout_probs\": [0, 0],\n","}\n","\n","args_dict = {\n","\t\"filename\": filename, \n","\t\"model_id\": \"Conv_5_layer\",\n","\t\"num_epochs\": 100,\n","\t\"interval\": 16,\n","\t#\"lr\": 0.001,\n","\t\"batch_size\": 64,\n","\t\"val_split\": 0.2,\n","\t\"save_checkpoint\": False,\n","\t\"criterion\": nn.CrossEntropyLoss(),\n","\t\"patience\": 2,\n","\t\"min_epochs\": 3,\n","\t\"buffer\": 0.05,\n","  \"lr\": 0.01,\n","\t\"model_args\": model_args,\n","  'checkpoint_path': CHECKPOINT_ROOT,\n","}\n","\n","batch_sizes = [4, 8, 16, 32, 64, 128, 256]\n","for batch in batch_sizes:\n","  args_dict['checkpoint_name'] = \"conv_net_batch_{}.pt\".format(batch)\n","  args_dict['batch_size'] = batch \n","  args_dict['notes'] = dedent(\"\"\"\n","  training conv net while varying batch size.\n","  batch size = {}\n","\n","  other hyperparams: \n","  id: Conv_5_layer\n","  max_epochs: 100\n","  interval: 16\n","  learning_rate: 0.01\n","  val_split: 0.2\n","  criterion: CrossEntropyLoss\n","  \"\"\".format(batch))\n","  args_dict['experiment_params'] = {\n","    'batch_size': batch\n","  }\n","  train_model(**args_dict)"],"execution_count":7,"outputs":[{"output_type":"stream","text":["device:  cpu\n","dataset metadata:  {'n_fft': 2048, 'hop_length': 256, 'n_mfcc': 13, 'notes': 'optional notes not passed'}\n","train model: data loaders initialized\n","sample shape =  (1, 13, 517)\n","model loaded\n","==========================================================================================\n","Layer (type:depth-idx)                   Output Shape              Param #\n","==========================================================================================\n","├─ConvBlock: 1-1                         [-1, 8, 6, 258]           --\n","|    └─Sequential: 2-1                   [-1, 8, 6, 258]           --\n","|    |    └─Conv2d: 3-1                  [-1, 8, 13, 517]          80\n","|    |    └─ReLU: 3-2                    [-1, 8, 13, 517]          --\n","|    |    └─MaxPool2d: 3-3               [-1, 8, 6, 258]           --\n","|    |    └─BatchNorm2d: 3-4             [-1, 8, 6, 258]           16\n","├─ConvBlock: 1-2                         [-1, 8, 3, 129]           --\n","|    └─Sequential: 2-2                   [-1, 8, 3, 129]           --\n","|    |    └─Conv2d: 3-5                  [-1, 8, 6, 258]           584\n","|    |    └─ReLU: 3-6                    [-1, 8, 6, 258]           --\n","|    |    └─MaxPool2d: 3-7               [-1, 8, 3, 129]           --\n","|    |    └─BatchNorm2d: 3-8             [-1, 8, 3, 129]           16\n","├─ConvBlock: 1-3                         [-1, 32, 1, 64]           --\n","|    └─Sequential: 2-3                   [-1, 32, 1, 64]           --\n","|    |    └─Conv2d: 3-9                  [-1, 32, 3, 129]          2,336\n","|    |    └─ReLU: 3-10                   [-1, 32, 3, 129]          --\n","|    |    └─MaxPool2d: 3-11              [-1, 32, 1, 64]           --\n","|    |    └─BatchNorm2d: 3-12            [-1, 32, 1, 64]           64\n","├─ConvBlock: 1-4                         [-1, 32, 1, 32]           --\n","|    └─Sequential: 2-4                   [-1, 32, 1, 32]           --\n","|    |    └─Conv2d: 3-13                 [-1, 32, 1, 64]           9,248\n","|    |    └─ReLU: 3-14                   [-1, 32, 1, 64]           --\n","|    |    └─MaxPool2d: 3-15              [-1, 32, 1, 32]           --\n","|    |    └─BatchNorm2d: 3-16            [-1, 32, 1, 32]           64\n","├─ConvBlock: 1-5                         [-1, 64, 1, 16]           --\n","|    └─Sequential: 2-5                   [-1, 64, 1, 16]           --\n","|    |    └─Conv2d: 3-17                 [-1, 64, 1, 32]           18,496\n","|    |    └─ReLU: 3-18                   [-1, 64, 1, 32]           --\n","|    |    └─MaxPool2d: 3-19              [-1, 64, 1, 16]           --\n","|    |    └─BatchNorm2d: 3-20            [-1, 64, 1, 16]           128\n","├─LinearBlock: 1-6                       [-1, 128]                 --\n","|    └─Sequential: 2-6                   [-1, 128]                 --\n","|    |    └─Linear: 3-21                 [-1, 128]                 131,200\n","|    |    └─ReLU: 3-22                   [-1, 128]                 --\n","|    |    └─BatchNorm1d: 3-23            [-1, 128]                 256\n","|    |    └─Dropout: 3-24                [-1, 128]                 --\n","├─LinearBlock: 1-7                       [-1, 64]                  --\n","|    └─Sequential: 2-7                   [-1, 64]                  --\n","|    |    └─Linear: 3-25                 [-1, 64]                  8,256\n","|    |    └─ReLU: 3-26                   [-1, 64]                  --\n","|    |    └─BatchNorm1d: 3-27            [-1, 64]                  128\n","|    |    └─Dropout: 3-28                [-1, 64]                  --\n","├─HeadBlock: 1-8                         [-1, 11]                  --\n","|    └─Sequential: 2-8                   [-1, 11]                  --\n","|    |    └─Linear: 3-29                 [-1, 11]                  715\n","==========================================================================================\n","Total params: 171,587\n","Trainable params: 171,587\n","Non-trainable params: 0\n","Total mult-adds (M): 3.93\n","==========================================================================================\n","Input size (MB): 0.03\n","Forward/backward pass size (MB): 0.78\n","Params size (MB): 0.65\n","Estimated Total Size (MB): 1.46\n","==========================================================================================\n","criterion:  CrossEntropyLoss()\n","\n","\n","training epoch:  0\n","model set to train\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:110: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"],"name":"stderr"},{"output_type":"stream","text":["Epoch [1/100], step [1/1341],  Loss: 2.6751 +/- 0.0000,  accuracy: 0.0, time elapsed = 0.34514570236206055s\n","Epoch [1/100], step [17/1341],  Loss: 2.6984 +/- 0.3992,  accuracy: 0.0625, time elapsed = 0.584130048751831s\n","Epoch [1/100], step [33/1341],  Loss: 2.5855 +/- 0.3525,  accuracy: 0.171875, time elapsed = 0.6200547218322754s\n","Epoch [1/100], step [49/1341],  Loss: 2.5157 +/- 0.4183,  accuracy: 0.109375, time elapsed = 0.5774974822998047s\n","Epoch [1/100], step [65/1341],  Loss: 2.6046 +/- 0.2266,  accuracy: 0.09375, time elapsed = 0.604456901550293s\n","Epoch [1/100], step [81/1341],  Loss: 2.2449 +/- 0.3221,  accuracy: 0.265625, time elapsed = 0.5589182376861572s\n","Epoch [1/100], step [97/1341],  Loss: 2.6985 +/- 0.4941,  accuracy: 0.0625, time elapsed = 0.570319652557373s\n","Epoch [1/100], step [113/1341],  Loss: 2.6245 +/- 0.3991,  accuracy: 0.09375, time elapsed = 0.573786735534668s\n","Epoch [1/100], step [129/1341],  Loss: 2.4393 +/- 0.4175,  accuracy: 0.171875, time elapsed = 0.5834426879882812s\n","Epoch [1/100], step [145/1341],  Loss: 2.3584 +/- 0.2884,  accuracy: 0.1875, time elapsed = 0.5844295024871826s\n"],"name":"stdout"},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-7-65bcab00193f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     50\u001b[0m     \u001b[0;34m'batch_size'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m   }\n\u001b[0;32m---> 52\u001b[0;31m   \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0margs_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-6-b1681a7d6a0a>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(filename, model_id, num_epochs, interval, lr, batch_size, val_split, save_checkpoint, checkpoint_path, notes, checkpoint_name, criterion, patience, min_epochs, buffer, dropout_prob, model_args, experiment_params)\u001b[0m\n\u001b[1;32m    102\u001b[0m     \u001b[0mtrain_preds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m     \u001b[0mtrain_targets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 104\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m       \u001b[0;31m# prep input and target tensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    515\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    516\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 517\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    518\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    519\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1180\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1181\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_shutdown\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1182\u001b[0;31m             \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1183\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1184\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_get_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1146\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1147\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1148\u001b[0;31m                 \u001b[0msuccess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1149\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0msuccess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1150\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    984\u001b[0m         \u001b[0;31m#   (bool: whether successfully get data, any: data if successful else None)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    985\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 986\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data_queue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    987\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    988\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.7/multiprocessing/queues.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    111\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_rlock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelease\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m         \u001b[0;31m# unserialize the data after having released the lock\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_ForkingPickler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mqsize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/multiprocessing/reductions.py\u001b[0m in \u001b[0;36mrebuild_storage_fd\u001b[0;34m(cls, df, size)\u001b[0m\n\u001b[1;32m    280\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    281\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mrebuild_storage_fd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 282\u001b[0;31m     \u001b[0mfd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    283\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    284\u001b[0m         \u001b[0mstorage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstorage_from_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfd_id\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.7/multiprocessing/resource_sharer.py\u001b[0m in \u001b[0;36mdetach\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m             \u001b[0;34m'''Get the fd.  This should only be called once.'''\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m             \u001b[0;32mwith\u001b[0m \u001b[0m_resource_sharer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_connection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_id\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mconn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_handle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.7/multiprocessing/resource_sharer.py\u001b[0m in \u001b[0;36mget_connection\u001b[0;34m(ident)\u001b[0m\n\u001b[1;32m     85\u001b[0m         \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mconnection\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mClient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m         \u001b[0maddress\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mident\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m         \u001b[0mc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mClient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maddress\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mauthkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcurrent_process\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauthkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m         \u001b[0mc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetpid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.7/multiprocessing/connection.py\u001b[0m in \u001b[0;36mClient\u001b[0;34m(address, family, authkey)\u001b[0m\n\u001b[1;32m    496\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    497\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mauthkey\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 498\u001b[0;31m         \u001b[0manswer_challenge\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mauthkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    499\u001b[0m         \u001b[0mdeliver_challenge\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mauthkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    500\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.7/multiprocessing/connection.py\u001b[0m in \u001b[0;36manswer_challenge\u001b[0;34m(connection, authkey)\u001b[0m\n\u001b[1;32m    744\u001b[0m     \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCHALLENGE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    745\u001b[0m     \u001b[0mdigest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhmac\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnew\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mauthkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'md5'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdigest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 746\u001b[0;31m     \u001b[0mconnection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_bytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdigest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    747\u001b[0m     \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconnection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_bytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m256\u001b[0m\u001b[0;34m)\u001b[0m        \u001b[0;31m# reject large message\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    748\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mresponse\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mWELCOME\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.7/multiprocessing/connection.py\u001b[0m in \u001b[0;36msend_bytes\u001b[0;34m(self, buf, offset, size)\u001b[0m\n\u001b[1;32m    185\u001b[0m         \u001b[0mm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmemoryview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m         \u001b[0;31m# HACK for byte-indexing of non-bytewise buffers (e.g. array.array)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 187\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitemsize\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    188\u001b[0m             \u001b[0mm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmemoryview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m         \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"code","metadata":{"id":"NVvyfWWlzOWf"},"source":[""],"execution_count":null,"outputs":[]}]}